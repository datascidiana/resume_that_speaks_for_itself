{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import (Settings, VectorStoreIndex, SimpleDirectoryReader, PromptTemplate)\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "import chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model=\"llama3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Diana\\Documents\\Code\\RAG\\ollama\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import documents and read them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(input_files=[\"./Diana_Morales_Resume.pdf\"]).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client = chromadb.EphemeralClient()\n",
    "chroma_collection = chroma_client.create_collection(\"ollama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_documents(documents, \n",
    "                                        storage_context=storage_context, \n",
    "                                        embed_model=embed_model,\n",
    "                                        transformations=[SentenceSplitter(chunk_size=256, chunk_overlap=10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custome prompt template\n",
    "template = (\n",
    "    \"Imagine you are a data scientit's assistant and you answer recruiter's questions about the data scientist's experience.\"\n",
    "    \"Here is some context from the data scientist's resume related to the query::\\n\"\n",
    "    \"-----------------------------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"-----------------------------------------\\n\"\n",
    "    \"Considering the above information, please respond to the following inquiry:\\n\\n\"\n",
    "    \"Question: {query_str}\\n\\n\"\n",
    "    \"Answer succinctly and ensure your response is understandable to someone without data science background.\"\n",
    "    \"The data scientist's name is Diana.\"\n",
    ")\n",
    "qa_template = PromptTemplate(template)\n",
    "\n",
    "# build query engine with custom template\n",
    "# text_qa_template specifies custom template\n",
    "# similarity_top_k configure the retriever to return the top 3 most similar documents,\n",
    "# the default value of similarity_top_k is 2\n",
    "query_engine = index.as_query_engine(text_qa_template=qa_template, similarity_top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"Do you have experience with Python?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes, Diana has experience working with Python. According to her resume, Python is listed as one of her skills under the \"Skills\" section. This suggests that she has some proficiency in using Python for various tasks, possibly including data analysis, machine learning, and more.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
